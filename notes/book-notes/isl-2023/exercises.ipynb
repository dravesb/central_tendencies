{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88387c99",
   "metadata": {},
   "source": [
    "# Chapter 2 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fcc1c8",
   "metadata": {},
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfd3937",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Would a flexible model be better or worse than an inflexible model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3b816",
   "metadata": {},
   "source": [
    "For all of these questions this is a bias variance tradeoff question. Inflexible methods are low variance and high bias. The setting we want to understand which is a greater risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeea9d0",
   "metadata": {},
   "source": [
    "a. Large n p is small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf718d",
   "metadata": {},
   "source": [
    "Here since n is so large, we can drive the bias to zero pretty easily with a very flexible model as long as we dont overfit. So here is a good opportunity to use a flexible model to get that bias term as long as possible. We need to ensure we dont overfit though in case the variance will drive the test MSE too high."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce47f3a",
   "metadata": {},
   "source": [
    "b. p is large and n is small"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130573ae",
   "metadata": {},
   "source": [
    "Here driving the bias to zero will be very hard since we have so few examples. The variance for inflexible models will be lower than an inflexible model however, since there will only be a few model configurations it can take up. So we choose the inflexible model as its less variance prone and both methods will suffer bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8b5152",
   "metadata": {},
   "source": [
    "c. Relationships between predictors is highly nonlinear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52e4cd4",
   "metadata": {},
   "source": [
    "Need a flexible model to capture those interdependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0985e1",
   "metadata": {},
   "source": [
    "d. Either model type can work - we can't reproduced this error. In partice, complex models tend to overfit howwever (trying to attribute this high noise ratio to signal) so inflexible models might be a better choice to ensure we are not exposing ourselves to variance risks but in general either method can work in this scenario as this is irreducible error. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f588ac8",
   "metadata": {},
   "source": [
    "2. Inference or Prediction / Regression or Classification? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190cedd5",
   "metadata": {},
   "source": [
    "a. Regression - inference \n",
    "\n",
    "b. Classification - prediction \n",
    "\n",
    "c. Regession - prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc390bcd",
   "metadata": {},
   "source": [
    "3. Bias Variance decomp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c527dc2",
   "metadata": {},
   "source": [
    "Provide a plot of \n",
    "\n",
    "1. squared bias \n",
    "2. variance \n",
    "3. training error \n",
    "4. test error\n",
    "5. bayes (irreducible) error curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d539748",
   "metadata": {},
   "source": [
    "bias - as we go from less to more flexible, the training error drops quadratically to zero.\n",
    "\n",
    "variance - as we go from less to more flexible, the variance goes from low to higher, shaped upward \n",
    "\n",
    "training error - sum the first two lines. Should generally be decreasing to zero as we drive out bias from the set. \n",
    "\n",
    "test error - a bowl that represents over / under fitting. Should inflect at the irreducbline error line\n",
    "\n",
    "irreducible error - flat line "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e6792",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cb26bcd",
   "metadata": {},
   "source": [
    "4. \n",
    "\n",
    "a - disese prediction, economic up/down turn, train late/on time\n",
    "\n",
    "b - salaray prediction, GDP increase / decrease, training wait time \n",
    "\n",
    "c - coworker salary comps, soccer recruitment, friend group recommendation on socials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff80da12",
   "metadata": {},
   "source": [
    "5. Flexible approaches are great at prediction and hard to infer relationships from. Inflexible approaches are more inteprertable and more robust to training set noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65848310",
   "metadata": {},
   "source": [
    "6. Parametric modeling we enforce the model to take a specific shape - limiting its flexibility to a subset of functional forms. Non-parameteric methods do not make this assumption and tend to be more flexible with the data they see. \n",
    "\n",
    "Parametric modeling tends to be more predictable so we can study them more in depth than non-parametric models which are more flexible but can be more difficult to utilize in infernece settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31ac824",
   "metadata": {},
   "source": [
    "7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d94e9",
   "metadata": {},
   "source": [
    "a. compute euclidean distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c19c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as npa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0631198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0, 3, 0], [2, 0, 0], [0, 1, 3], [0, 1, 2], [-1, 0, 1], [1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "071b233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([\"Red\", \"Red\", \"Red\", \"Green\", \"Green\", \"Red\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28b81524",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.array([0, 0, 0])\n",
    "\n",
    "dist = [np.sqrt(np.sum((x - z) ** 2)) for x in X]\n",
    "neighbors = dict(zip(list(range(X.shape[0])), dist))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7afdd",
   "metadata": {},
   "source": [
    "b/c - predictions on NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdef1e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Green'], dtype='<U5')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 1\n",
    "Y[np.argsort(dist)[:K].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a66bc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Green', 'Red', 'Red'], dtype='<U5')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = 3\n",
    "Y[np.argsort(dist)[:K].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7475f",
   "metadata": {},
   "source": [
    "d. If its highly non linear, than we need a more flexible model to attain that boundary which requires a smaller K."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ffdcb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
