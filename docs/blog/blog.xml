<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Central Tendencies</title>
<link>dravesb.github.io/blog/blog.html</link>
<atom:link href="dravesb.github.io/blog/blog.xml" rel="self" type="application/rss+xml"/>
<description></description>
<generator>quarto-1.8.26</generator>
<lastBuildDate>Sun, 01 Dec 2024 05:00:00 GMT</lastBuildDate>
<item>
  <title>Item-Based Collaborative Filtering Recommendation Algorithms</title>
  <dc:creator>Benjamin Draves</dc:creator>
  <link>dravesb.github.io/blog/blog_posts/sarwar_2001/</link>
  <description><![CDATA[ 





<section id="overview" class="level1">
<h1>Overview</h1>
<p><a href="https://files.grouplens.org/papers/www10_sarwar.pdf">Sarwar et al.’s 2001 paper on Item-Based Collaborative Filtering</a> is one of the first papers from the GroupLens group on estimating user-item ratings utilizing item-to-item similarities. This paper discusses an item based similarity rating scheme and much of the computational benefits one can gain from utilizing the item-based approach and a model rating estimation framework. Moreover, the authors provide thorough experiments which address the impact of data sparsity, neighborhood size, and similarity measures on the performance of this algorithm.</p>
</section>
<section id="introduction-problem" class="level1">
<h1>Introduction &amp; Problem</h1>
<p>The key challenges that the authors attempt to address in the paper are (i) improve the scalability of user-to-user collaborative filtering algorithms and (ii) improve the quality of the recommendations produced by these algorithms. They give an overview of existing collaborative filtering approaches which tend to be very memory intensive and non-scalable. For example, consider the rating scheme for user-to-user collaborative filtering.</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BR%7D_%7Bus%7D%20=%20%5Cbar%7BR%7D_u%20+%20%5Cfrac%7B%5Csum_%7Bv%5Cin%20N(u)%7D%5Ctext%7Bsim%7D(u,%20v)(R_%7Bvs%7D%20-%20%5Cbar%7BR%7D_v)%7D%7B%5Csum_%7Bv%5Cin%20N(u)%7D%5Cmid%5Ctext%7Bsim%7D(u,v)%5Cmid%7D%0A"></p>
<p>This ratings approach is both non-scalable and may suffer from data sparsity.</p>
<p>To see why it is non-scalable, consider a given user <img src="https://latex.codecogs.com/png.latex?u">. The construction of the neighborhood <img src="https://latex.codecogs.com/png.latex?N(u)"> is typically performed during online-inference as the the number of co-rated items between users can change quickly (e.g.&nbsp;a new user who ranks 10 items upon sign-up). Therefore, this computation must be done in real-time or face some SLA, necessitating a full loop over the user based <img src="https://latex.codecogs.com/png.latex?O(n)"> for each user-item pair we look to estimate a rating for.</p>
<p>To see why this method suffers from data sparsity, consider if a user has no neighbors <img src="https://latex.codecogs.com/png.latex?N(u)"> that have rated item <img src="https://latex.codecogs.com/png.latex?s">. In this setting the numerator is ill-defined and user-to-user based CF cannot provide recommendations.</p>
<p>These observations motivate the authors to abandon measuring the very costly <img src="https://latex.codecogs.com/png.latex?O(n%5E2)"> user-to-user similarity problem during online inference in favor of the <img src="https://latex.codecogs.com/png.latex?O(m%5E2)"> item-to-item similarity problem which can be carried out offline. The intuition is that users will enjoy products similar to what they have enjoyed in the past, and that item-to-item similarity is a more stable relationship over time, allowing for large batch computations of these scores.</p>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<section id="item-based-collaborative-filtering" class="level2">
<h2 class="anchored" data-anchor-id="item-based-collaborative-filtering">Item-Based Collaborative Filtering</h2>
<p>Similar to user-to-user based collaborative filtering, ratings can be estimated using either (i) a <em>memory based</em> score utilizing weighted sums of observed ratings or (ii) a <em>model based</em> score using rules <em>learned</em> from the observed ratings data. For the memory based approach, ratings are estimated as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BR%7D_%7Bus%7D%20=%20%5Cfrac%7B%5Csum_%7Bt%5Cin%20N(s)%7D%5Ctext%7Bsim%7D(s,%20t)R_%7But%7D%7D%7B%5Csum_%7Bt%5Cin%20N(s)%7D%5Cmid%5Ctext%7Bsim%7D(s,%20t)%5Cmid%7D%0A"></p>
<p>where the neighborhood <img src="https://latex.codecogs.com/png.latex?N(s)"> is the set of all items that share a co-rater (e.g.&nbsp;someone who rated both item <img src="https://latex.codecogs.com/png.latex?s"> and <img src="https://latex.codecogs.com/png.latex?t">). Notice here we do not need to mean center user <img src="https://latex.codecogs.com/png.latex?u">’s ratings as we only utilize their ratings in this computation. Moreover, we can pre-compute the neighborhood <img src="https://latex.codecogs.com/png.latex?N(s)"> and the item-to-item similarities offline improving performance.</p>
<p>In addition to this scoring methodology, the authors introduce a regression-based model approach. They utilizing the same scoring function above by utilize the smoothed scores <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7BR%7D_%7But%7D"> by first regressing the item vectors <img src="https://latex.codecogs.com/png.latex?R_%7B%5Ccdot,%20t%7D"> onto <img src="https://latex.codecogs.com/png.latex?R_%7B%5Ccdot,%20s%7D">. (Candidly, I found this section very hard to follow. I think <a href="https://link.springer.com/book/10.1007/978-3-319-29659-3">Aggarwal’s Section 2.6</a> connects these ideas more clearly.)</p>
</section>
<section id="measuring-item-similarity" class="level2">
<h2 class="anchored" data-anchor-id="measuring-item-similarity">Measuring Item Similarity</h2>
<p>Measuring item-to-item similarity is the key to solving the collaborative filtering problem. The authors introduce three methods which are still popular today. For ease of notation, let <img src="https://latex.codecogs.com/png.latex?r_s%5Cin%5Cmathbb%7BR%7D%5EN"> denote the ratings vector for item <img src="https://latex.codecogs.com/png.latex?s">. Then the three methods are given by</p>
<ol type="1">
<li>Cosine similarity: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Ctext%7Bsim%7D(s,%20t)%20=%20%5Cfrac%7Br_s%5ETr_t%7D%7B%5C%7Cr_s%5C%7C_2%5C%7Cr_t%5C%7C_2%7D%0A%20%20"></li>
<li>Adjusted-cosine similarity: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Ctext%7Bsim%7D(s,%20t)%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5En(R_%7Bis%7D%20-%20%5Cbar%7BR%7D_i)(R_%7Bit%7D%20-%20%5Cbar%7BR%7D_i)%7D%7B%5Csqrt%7B%5Csum_%7Bi=1%7D%5En(R_%7Bis%7D%20-%20%5Cbar%7BR%7D_i)%5E2%7D%5Csqrt%7B%5Csum_%7Bi=1%7D%5En(R_%7Bit%7D%20-%20%5Cbar%7BR%7D_i)%5E2%7D%7D%0A%20%20"></li>
<li>Correlation similarity: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%5Ctext%7Bsim%7D(s,%20t)%20=%20%5Cfrac%7B%5Csum_%7Bi=1%7D%5En(R_%7Bis%7D%20-%20%5Cbar%7BR%7D_s)(R_%7Bit%7D%20-%20%5Cbar%7BR%7D_t)%7D%7B%5Csqrt%7B%5Csum_%7Bi=1%7D%5En(R_%7Bis%7D%20-%20%5Cbar%7BR%7D_s)%5E2%7D%5Csqrt%7B%5Csum_%7Bi=1%7D%5En(R_%7Bit%7D%20-%20%5Cbar%7BR%7D_t)%5E2%7D%7D%0A%20%20"></li>
</ol>
<p>The unifying piece of each of these similarity methods is the cosine similarity operator. The only thing that changes between these methods is which matrix of ratings it acts on.</p>
<p>For instance, taking <img src="https://latex.codecogs.com/png.latex?%5Cell_2">-normalized dot products of column vectors in <img src="https://latex.codecogs.com/png.latex?R"> gives the cosine similarity. The same operation on the mean-centered rows gives the adjusted cosine similarity and mean-center columns give the correlation similarity.</p>
<p>Therefore, I think it’s better to think about each of these similarity measures as performing the same operation (i.e.&nbsp;cosine-similarity between column vectors) just on difference matrices:</p>
<ol type="1">
<li>Cosine similarity: <img src="https://latex.codecogs.com/png.latex?R"></li>
<li>Adjusted-cosine similarity: <img src="https://latex.codecogs.com/png.latex?R%20-%20(R%5Cmathbf%7B1%7D_m%5Cmathbf%7B1%7D_m%5ET)/m"></li>
<li>Correlation similarity: <img src="https://latex.codecogs.com/png.latex?R%20-%20(%5Cmathbf%7B1%7D_n%5Cmathbf%7B1%7D_n%5ETR)/n"></li>
</ol>
<p>I’m ignoring the fact that some ratings here are not-present and writing <img src="https://latex.codecogs.com/png.latex?1/m"> and <img src="https://latex.codecogs.com/png.latex?1/n"> for normalizing constants, but the spirit holds.</p>
</section>
</section>
<section id="ramifications" class="level1">
<h1>Ramifications</h1>
<section id="compute-performance" class="level2">
<h2 class="anchored" data-anchor-id="compute-performance">Compute Performance</h2>
<p>The authors spend most of their attention not on measuring the differences between model based and memory based approaches (they do concede that model based approaches are more scalable in the long run) and more so on separating the neighborhood construction and the ratings estimation in offline and online batches, respectively.</p>
<p>They authors recommend that instead of computing the full neighborhood, only the top <img src="https://latex.codecogs.com/png.latex?k"> most similar items are retained (stochastic gradient descent vibes). They call this parameter <img src="https://latex.codecogs.com/png.latex?k"> the <em>model size</em> or the <em>neighborhood size</em>.</p>
<p>The authors cary out a number of experimental examples and show that</p>
<ol type="1">
<li><p>The memory based method outperforms the regression based method and demonstrate overfitting when the neighborhood size gets too large: <img src="dravesb.github.io/blog/blog_posts/sarwar_2001/neighborhood_size.jpg" class="img-fluid" alt="Figure: Comparing model performance. Figure 5 in Sarwar 2001"></p></li>
<li><p>The algorithm only needs the most similar items in an item’s neighborhood to estimate ratings accurately. In the paper they state “… we were within 96% and 98.3% of the full item-item scheme’s accuracy using only 1.3% and 3.0% of the items, respectively!” <img src="dravesb.github.io/blog/blog_posts/sarwar_2001/model_size.jpg" class="img-fluid" alt="Figure: Effect of neighborhood size. Figure 7 in Sarwar 2001"></p></li>
</ol>
<p>These findings aren’t too surprising but do garner insights into ways to speed up this algorithm in online inference settings and again demonstrates bias-variance tradeoffs in offline model training.</p>
</section>
<section id="recommendation-performance" class="level2">
<h2 class="anchored" data-anchor-id="recommendation-performance">Recommendation Performance</h2>
<p>The paper mostly focused on assessing the algorithms performance with respect to MAE utilizing a train-test split. For collaborative filtering approaches, I also find this a bit concerning because severing the user-item bipartite graph impacts training of all collaborative filtering methods. Nonetheless, the authors carry out a classic training-testing split, and cross validate on the training set to tune the item-to-item collaborative filtering methods. Their key findings include</p>
<ol type="1">
<li>Adjusted-cosine similarity out performs other similarity methods</li>
<li>After tuning, item-to-item give similar, if not preferable, results compared to user-to-user methods for certain combinations of data sparsity and neighborhood sizes.</li>
</ol>
<p>In aggregate, I think these results validate the performance of this method as a valid and interesting alternative to user-to-user collaborative filtering.</p>
</section>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>In conclusion, the paper introduces item-to-item collaborative filtering as a method that gives similar performance as user-to-user collaborative filtering with potential gains in performance by moving much of the heavy computation to offline batch frameworks. Moreover, by proposing modeling techniques that do not use the training data explicitly in the creation of the rating estimates (i.e.&nbsp;model based methods) these methods will continue to offer compute enhancements.</p>
<p>I think this paper offers a nice introduction to a different view of collaborative filtering while stressing some of the practical limitations of neighborhood approaches in real-time systems. A few thoughts I have leaving the paper:</p>
<ul>
<li><p>This naturally begs the question of utilizing both user level and item level neighborhoods when performing localized-averaging for ratings estimation. Something along the lines of <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BR%7D_%7Bus%7D%20=%20%5Cfrac%7B%5Csum_%7Bt%5Cin%20N(s)%7D%5Csum_%7Bv%5Cin%20N(u)%7D%5Ctext%7Bsim%7D(v,%20t)R_%7Bvt%7D%7D%7B%5Csum_%7Bt%5Cin%20N(s)%7D%5Csum_%7Bv%5Cin%20N(u)%7D%5Cmid%20%5Ctext%7Bsim%7D(v,%20t)%5Cmid%7D%0A"> which is starting to look very page-rank.</p></li>
<li><p>None of these papers are utilizing user or item features at all. These similarity measures could easily utilize features on each item which would greatly improve the quality of these measures.</p></li>
<li><p>Performing training-testing splits on graph data remain very difficult.</p></li>
</ul>


</section>

 ]]></description>
  <category>paper-notes</category>
  <category>machine-learning</category>
  <category>recommendation-systems</category>
  <category>collaborative-filtering</category>
  <guid>dravesb.github.io/blog/blog_posts/sarwar_2001/</guid>
  <pubDate>Sun, 01 Dec 2024 05:00:00 GMT</pubDate>
</item>
<item>
  <title>GroupLens: An Open Architecture for Collaborative Filtering of Netnews</title>
  <dc:creator>Benjamin Draves</dc:creator>
  <link>dravesb.github.io/blog/blog_posts/resnick_1994/</link>
  <description><![CDATA[ 





<section id="overview" class="level1">
<h1>Overview</h1>
<p>The <a href="http://dx.doi.org/10.1145/192844.192905"><em>GroupLens: An Open Architecture for Collaborative Filtering of Netnews</em></a> paper was one of the first papers to introduce collaborative filtering for recommendation applications in 1994. The paper focuses on not just the recommendation problem but also how those recommendations are integrated into the Usenet Netnews network, displayed to users, and implications for serving personalized news to users.</p>
</section>
<section id="introduction-problem" class="level1">
<h1>Introduction &amp; Problem</h1>
<p>Usenet’s netnews platform was an original social news aggregation network. Netnews created and shared bulletin boards where users could post news related to certain topics (see the figure below). This mimics many modern day social media platforms (most notably Reddit). While users could see and read news posted from other users, users noted that the “signal to noise” ratio was too low and had to rely on moderators and basic software (called news agents) to screen, remove, or boost certain news content.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="dravesb.github.io/blog/blog_posts/resnick_1994/netnews_architecture.jpg" class="img-fluid figure-img"></p>
<figcaption><em>Figure</em>: Netnews design and architecture. Figure 1 in Resnick et al.&nbsp;1994</figcaption>
</figure>
</div>
<p>While some of this was successful, most of these approaches were primitive and would not scale to many users and articles. GroupLens attempted to create a system (a news client) that would take into account user ratings to <em>collaboratively</em> filter our irrelevant news articles bases on the uses ratings of previous articles.<br>
The stated goals of GroupLens was to</p>
<ol type="1">
<li>Openess - other devs could change the rating system to address their own objectives.</li>
<li>Ease of use - users could supply ratings to news articles was non-invasive</li>
<li>Compatibility - the news client was accessible to all Netnews users</li>
<li>Scalability - the rating mechanism scaled to many users and many articles</li>
<li>Privacy - ensure users ratings privacy were preserved</li>
</ol>
</section>
<section id="methods" class="level1">
<h1>Methods</h1>
<p>The paper spend a good deal of time discussing the architecture of <em>how</em> article ratings will get passed from server to server to influence filtering of articles for new users (see Figure 2 for instance). The authors also spend time discussing the tradeoffs from building GroupLens on explicit ratings (e.g.&nbsp;user scores on each article from 1-5) versus implicit ratings (e.g.&nbsp;time spend reading each article). In modern applications, most recommendation systems focus on incorporating both explicit and implicit feedback but tend to focus on the later due to higher throughput and reliability of implicit feedback mechanisms (e.g.&nbsp;<a href="https://support.tiktok.com/en/using-tiktok/exploring-videos/how-tiktok-recommends-content">TikTok’s dwell time</a>).</p>
<p>The authors ultimately decide to focus on explicit article ratings and treated the recommendation problem as a matrix completion problem.</p>
<section id="collaborative-filtering" class="level2">
<h2 class="anchored" data-anchor-id="collaborative-filtering">Collaborative Filtering</h2>
<p>For each user <img src="https://latex.codecogs.com/png.latex?u%5Cin%5Cmathcal%7BU%7D"> and item <img src="https://latex.codecogs.com/png.latex?v%5Cin%5Cmathcal%7BV%7D"> we assign the rating <img src="https://latex.codecogs.com/png.latex?u"> assigns to <img src="https://latex.codecogs.com/png.latex?v"> as <img src="https://latex.codecogs.com/png.latex?R_%7Buv%7D">. Supposing <img src="https://latex.codecogs.com/png.latex?%5Cmid%5Cmathcal%7BU%7D%5Cmid%20=%20n"> and <img src="https://latex.codecogs.com/png.latex?%5Cmid%5Cmathcal%7BV%7D%5Cmid%20=%20m"> the ratings matrix <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D%5Cin%5Cmathbb%7BR%7D%5E%7Bn%5Ctimes%20m%7D"> with</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbf%7BR%7D%20=%20%5Cbegin%7Bbmatrix%7D%0AR_%7B11%7D%20&amp;%20%5Cdots%20&amp;%20R_%7B1m%7D%5C%5C%0AR_%7B21%7D%20&amp;%20%5Cdots%20&amp;%20R_%7B2m%7D%5C%5C%0A%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%5C%5C%0AR_%7Bn1%7D%20&amp;%20%5Cdots%20&amp;%20R_%7Bnm%7D%0A%5Cend%7Bbmatrix%7D%0A"></p>
<p>Now, several entries in <img src="https://latex.codecogs.com/png.latex?%5Cmathbf%7BR%7D"> will be missing as not every (user, item) pair will be rated.</p>
<p>The authors impute this missing scores based on the hueristic that “people who agreed in the past are likely to agree again.” To impute the missing rating <img src="https://latex.codecogs.com/png.latex?R_%7Buv%7D"> they follow:</p>
<ol type="1">
<li>Understand how similar user <img src="https://latex.codecogs.com/png.latex?u"> is to all other users.</li>
<li>Score the item <img src="https://latex.codecogs.com/png.latex?v"> based on previous ratings of other users on <img src="https://latex.codecogs.com/png.latex?v"> <em>taking into account how similar the user is to <img src="https://latex.codecogs.com/png.latex?u"></em>.</li>
</ol>
<p>This manifests in the scoring equation <img src="https://latex.codecogs.com/png.latex?%0A%5Chat%7BR%7D_%7Buv%7D%20=%20%5Cbar%7BR_u%7D%20+%20%5Cfrac%7B%5Csum_%7Bs%5Cneq%20u%7D%20%5Ctext%7Bsim%7D(u,%20s)(R_%7Biv%7D%20-%20%5Cbar%7BR%7D_i)%7D%7B%5Csum_%7Bs%5Cneq%20u%7D%20%7C%5Ctext%7Bsim%7D(u,%20s)%7C%7D%0A"></p>
<p>Here, we first residualize all ratings relative to the user’s average rating. This removes any user-level bias or different interpretations of the rating scale. From here, we simply take a weighted sum of the ratings on item <img src="https://latex.codecogs.com/png.latex?v"> for each user, weighted by the similarity between the user providing that rating <img src="https://latex.codecogs.com/png.latex?i"> and user <img src="https://latex.codecogs.com/png.latex?u">. The authors propose to model user-to-user similarity using simple correlation scores:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Ctext%7Bsim%7D(u,%20s)%20=%20%5Cfrac%7B%5Csum_%7Bj=1%7D%5Em(R_%7Buj%7D%20-%20%5Cbar%7BR%7D_%7Bu%7D)(R_%7Bsj%7D%20-%20%5Cbar%7BR%7D_%7Bs%7D)%7D%7B%5Csqrt%7B%5Csum_%7Bj=1%7D%5Em(R_%7Buj%7D%20-%20%5Cbar%7BR%7D_%7Bu%7D)%5E2%7D%5Csqrt%7B%5Csum_%7Bj=1%7D%5Em(R_%7Bsj%7D%20-%20%5Cbar%7BR%7D_%7Bs%7D)%5E2%7D%7D%20%5Cin%20%5B-1,%201%5D%0A"></p>
<p>In both of these equations, if <img src="https://latex.codecogs.com/png.latex?R_%7Buv%7D"> is unobserved, then that term in the summand is removed. Therefore, if two users <img src="https://latex.codecogs.com/png.latex?(u,%20s)"> do not score any of the same items, their similarity is set to zero and user <img src="https://latex.codecogs.com/png.latex?s"> ratings do not impact the rating estimates for user <img src="https://latex.codecogs.com/png.latex?u">.</p>
<p>This approach is typically referred to user-collaborative filtering (since we measure user-to-user similarity) in modern applications and similar approaches for item-collaborative filtering</p>
</section>
<section id="collaborative-filtering---extensions" class="level2">
<h2 class="anchored" data-anchor-id="collaborative-filtering---extensions">Collaborative Filtering - Extensions</h2>
<p>The authors point out a number of limitations with this approach which are approached with newer methodologies but crucially they discuss:</p>
<ul>
<li>This method does not utilize implicit feedback</li>
<li>This method does not utilize features of the users or items</li>
<li>Similarity scoring across all users will not scale</li>
</ul>
</section>
</section>
<section id="ramifications" class="level1">
<h1>Ramifications</h1>
<p>The paper discuss how they might surface the confidence they have in the scores produced by this algorithm. The authors recommend the scores can be used to remove low (estimated) rated articles or can show the estimated scores as part of the UI. They point how each of these approaches may bias data collection but leave it to the News group moderators on how they want to integrate these ratings to their Newgroup bulletin.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>The paper spend a significant portion of time on the application (Netnews) and not the individual recommendation mechanism. This allows for moderators and users of this news client to understand what the recommendation collaborative filter is doing and how it can reliably add value over time. The authors even spend time understanding the social impact of fractured news ecosystems driven by recommendation engines. This quote is from their “Global Villages” section.</p>
<blockquote class="blockquote">
<p>Present newsgroups, like newspapers and local television shows before them, provide a shared history for their community of readers. With GroupLens, users may choose to read articles only from a small group with whom they share many common interests. Over time this could lead to a fracture of the global village into many small tribes, each forming a virtual community but nonetheless isolated from each other. Some kind of fracture is inevitable and even desirable, because no user can keep up with the overwhelming volume of news produced each day. The question is whether the subgroups will be closed or permeable.</p>
</blockquote>
<p>I think this paper does a great job of understanding the core application and developing a recommendation strategy for it instead of applying a recommendation strategy to a use case without thought. Perhaps that’s why it’s still relavent 30 years after publication.</p>


</section>

 ]]></description>
  <category>paper-notes</category>
  <category>machine-learning</category>
  <category>recommendation-systems</category>
  <category>collaborative-filtering</category>
  <guid>dravesb.github.io/blog/blog_posts/resnick_1994/</guid>
  <pubDate>Sun, 17 Nov 2024 05:00:00 GMT</pubDate>
</item>
</channel>
</rss>
