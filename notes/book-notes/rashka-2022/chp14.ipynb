{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 14: Classifying Images with Deep CNNs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chapter layout: \n",
    "\n",
    "1. Understanding Convolutions \n",
    "2. Implementing a CNN \n",
    "3. Implementing a Deep CNN with PyTorch \n",
    "4. Smile Classification using a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CNNs are a family of models that were inspired from the visual cortext of the human brain. \n",
    "    * Neurons responds differently to light: the primary layer detects edges, higher order layers detect shapes and patterns\n",
    "* Orginalally developed by Yann LeCun and colleagues in the 1990s\n",
    "* CNNs are usually refered to as _feature extraction layers_ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs are usually thought of as feature extraction layers that are able to extract low-level features in the early layers which are utilized later in the network to detect patterns between these features and the target variable of interest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNNs construct a *feature hierachy* by combining low-level features to form high-level features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN computes *feature maps* from an input image to create a feature. They look over _local receptive fields_ = local patch of pixels to construct and pool features in small areas of the image. \n",
    "\n",
    "This method relies on \n",
    "* sparse connectivity = an element of a feature map is only connected to its nearest neighbor \n",
    "* parameter sharing = by learning a common set of parameters in a patch, we can utilize it in different parts of the image \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically a CNN is constructed of \n",
    "\n",
    "1. Several _convolutional_ layers \n",
    "2. Subsampling / Pooling layers\n",
    "    * These don't have any weights or biases, just a useful aggregator\n",
    "3. Fully connected layers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A discrete convolution in 1D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A convolution for two discrete vectors is $x$ and $y$ is given by \n",
    "\n",
    "$$ z = x * y \\rightarrow z_i = \\sum_{k=\\infty}^{\\infty}x[i-k]y[k]$$\n",
    "\n",
    "where it is assumed $x[i] = y[j] = 0$ for all non-index elements of $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of filling these zeros is called _zero padding_. In addition to utilize the dot prodct it is typically to 'flip' the second vector and then dot it with the padded vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives the effect of \"sliding\" the smaller of the two vectors across it, and taking a localized weighted sum (dot product) to store in the convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This convolution has two hyperparamter \n",
    "\n",
    "1. Padding = how much additional zeros do we add to the vector to get\n",
    "2. Stride = how far down the vector do we shift to take the next product  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: This is like if we had a vector length 10 and we had a convolution of length 10, we'd only get a single number with no padding. By adding a bunch of zeros to the end of the vector, we can take local averages of the pixels toward the end of the arrays by themselves (e.g. [0, 1, 2] and [7, 8, 9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use padding and stride lengths to determine the length of the output of the convolution. The three most popular modes of padding are \n",
    "\n",
    "1. Full = $p = m-1$ this is super largr and starts so the first element $z[0] = x[0]$. For this reason, it's rarely used.\n",
    "2. Same = $p$ is selected so that the size of the output matches the input.\n",
    "3. Valid = $p=0$ ensures that there is no padding at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same padding is the most used in CNNs. It's typically to do a same padding CNNs layer followed by pooling or further convultional layers with higher stride lengths to decrease size for example: https://arxiv.org/abs/1412.6806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the size of the convolutional output layer, we need to understand how many times we shift the filter along the input vector. The fomula is given by \n",
    "\n",
    "$$ o = \\lfloor\\frac{n + 2p -m}{s} \\rfloor + 1 $$\n",
    "\n",
    "where \n",
    "* n = input size \n",
    "* p = padding size \n",
    "* m = filter size \n",
    "* s = stride length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive implementation is given below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def conv1d(x, y, p=0, s=1):\n",
    "    y_rot = np.array(y[::-1])\n",
    "    x_pad = np.array(x)\n",
    "    if p > 0:\n",
    "        zero_pad = np.zeros(shape=p)\n",
    "        x_pad = np.concatenate([zero_pad, x_pad, zero_pad])\n",
    "    res = []\n",
    "    for i in range(0, int((len(x_pad) - len(y_rot)) + 1), s):\n",
    "        # range(0, int((len(x_pad) - len(w_rot))) + 1, s)\n",
    "        res.append(np.sum(x_pad[i : i + y_rot.shape[0]] * y_rot))\n",
    "\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv1D Implementation: [ 5. 14. 16. 26. 24. 34. 19. 22.]\n"
     ]
    }
   ],
   "source": [
    "x = [1, 3, 2, 4, 5, 6, 1, 3]\n",
    "y = [1, 0, 3, 1, 2]\n",
    "\n",
    "print(\"Conv1D Implementation:\", conv1d(x, y, p=2, s=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy results: [ 5 14 16 26 24 34 19 22]\n"
     ]
    }
   ],
   "source": [
    "print(\"Numpy results:\", np.convolve(x, y, mode=\"same\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing 2D Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a two dimensional convoultion we have \n",
    "\n",
    "$$Z = X * Y \\rightarrow Z[i,j] = \\sum_{k_1=-\\infty}^{\\infty}\\sum_{k_2=-\\infty}^{\\infty}X[i-k_1, j-k_2]Y[k_1, k_2] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation has the effect of passing a filter matrix $Y$ over the matrix $X$ and creating small localized weighted means. This obviously has clear applications in image feature extraction where we want to locally pool pixels in an image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.signal\n",
    "\n",
    "\n",
    "def conv2d(X, Y, p=(0, 0), s=(1, 1)):\n",
    "    Y_rot = np.array(Y)[::-1, ::-1]\n",
    "    X_orig = np.array(X)\n",
    "\n",
    "    n1 = X_orig.shape[0] + 2 * p[0]\n",
    "    n2 = X_orig.shape[1] + 2 * p[1]\n",
    "\n",
    "    X_padded = np.zeros(shape=(n1, n2))\n",
    "    X_padded[p[0] : p[0] + X_orig.shape[0], p[1] : p[1] + X_orig.shape[1]] = X_orig\n",
    "\n",
    "    res = []\n",
    "    for i in range(0, int((X_padded.shape[0] - Y_rot.shape[0]) / s[0]) + 1, s[0]):\n",
    "        res.append([])\n",
    "        for j in range(0, int((X_padded.shape[1] - Y_rot.shape[1]) / s[1]) + 1, s[1]):\n",
    "            X_sub = X_padded[i : i + Y_rot.shape[0], j : j + Y_rot.shape[1]]\n",
    "            res[-1].append(np.sum(X_sub * Y_rot))\n",
    "\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1, 3, 2, 4], [5, 6, 1, 3], [1, 2, 0, 2], [3, 4, 3, 2]]\n",
    "Y = [[1, 0, 3], [1, 2, 1], [0, 1, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Implentation:\n",
      " [[11. 25. 32. 13.]\n",
      " [19. 25. 24. 13.]\n",
      " [13. 28. 25. 17.]\n",
      " [11. 17. 14.  9.]]\n"
     ]
    }
   ],
   "source": [
    "x = conv2d(X, Y, p=(1, 1), s=(1, 1))\n",
    "print(\"Conv2d Implentation:\\n\", x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy Implementation:\n",
      " [[11 25 32 13]\n",
      " [19 25 24 13]\n",
      " [13 28 25 17]\n",
      " [11 17 14  9]]\n"
     ]
    }
   ],
   "source": [
    "print(\"SciPy Implementation:\\n\", scipy.signal.convolve2d(X, Y, mode=\"same\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes on scaling \n",
    "\n",
    "* You can do these way faster by using fourier transform tricks \n",
    "* It typical to have a kernel that is much smaller than the input image \n",
    "    * Typical to have (1, 1), (3, 3) and (5,5) kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of pooling layers in CNNs which decrease the size of the feature set and reduces the dependence on one pixel in the image encouraging generalizability. \n",
    "\n",
    "1. max pooling = take the max in pixel neighborhood \n",
    "2. mean pooling = take the mean in pixel neighborhood \n",
    "\n",
    "And the neighborhod size (or pooling size) determines the size of the neighborhood. \n",
    "\n",
    "Max pooling is great for local heterogenity and robust to noise in the input data. \n",
    "Mean pooling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be used to reduce feature size OR another convolution layer with higher stride lengths. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the differences between these two aproaches see here: https://arxiv.org/abs/1412.6806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They key difference between tabular ANNs and CNNs is how we process the inputs: \n",
    "\n",
    "* ANN: $z_1 = Wx + b$ where $x$ is a vectorized input of pixels\n",
    "* CNN: $z_1 = W*X + b$ where $X$ is a matrix of pixels passed via a convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since pictures could be represented in multiple colors we need to understand how to process color channels. \n",
    "\n",
    "Each picture will be represented by a $n_1 \\times n_2 \\times c$ tensor where $c$ is the number of color channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([4, 360, 360])\n",
      "Number of channels: 4\n",
      "Image data type: torch.uint8\n"
     ]
    }
   ],
   "source": [
    "# reading in images\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "\n",
    "img = read_image(\"./figures/other/cat.png\")\n",
    "\n",
    "print(\"Image shape:\", img.shape)\n",
    "print(\"Number of channels:\", img.shape[0])\n",
    "print(\"Image data type:\", img.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[140, 132],\n",
      "         [129, 135]],\n",
      "\n",
      "        [[140, 132],\n",
      "         [129, 135]],\n",
      "\n",
      "        [[140, 132],\n",
      "         [129, 135]],\n",
      "\n",
      "        [[  0,   0],\n",
      "         [  0,   0]]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "print(img[:, 100:102, 100:102])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this how can we incorperate each channel into our convolution? We do it for each channel then add them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose each channel has it owns kernel matrix $W[::c]$ then the pre-activation inputs are given by \n",
    "\n",
    "$$Z^{(conv)} = \\sum_{c=1}^CW[:,:,c] * X[:, :, c] $$ \n",
    "$$Z = Z^{(conv)} + b$$\n",
    "$$A = \\sigma(Z)$$ \n",
    "\n",
    "Where this last matrix $A$ is called our feature map. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is typical for a CNN layer to have more than one feature map which is specificed by passing different kernels over each channel. That is: \n",
    "\n",
    "$$Z^{(conv)}[:,:,k] = \\sum_{c=1}^CW[:,:,c, k] * X[:, :, c]$$ \n",
    "$$Z[:,:,k] = Z^{(conv)}[:,:,k] + b[k]$$\n",
    "$$A[:,:,k] = \\sigma(Z[:,:,k)$$\n",
    "\n",
    "which supplies us with $K$ different feature maps which can be used in subsequent layers for learning. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the steps are as follows: \n",
    "\n",
    "1. For each kernel $K$, convolve over each channel $C$ and add them up. \n",
    "2. Do this for each kernel which provides $K$ new elements to work with.\n",
    "3. Perform max pooling with also results in $K$ elements but of possible smaller dimesion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: This was not super well explained. The point is that you can create individual feature maps by passing different kerels over each channel. Aggregating those feature maps serves as useful inputs for down the network. Morever, the network can learn what these kernels should be to maximize impact of these feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Regularization__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models truly have an insane number of weights so learning how to perform regularization seems really important. \n",
    "\n",
    "We'll disuss $\\ell_2$ regularization by regularizating the parameters in each layer and _dropout_ which randomly drops nodes from the network during training to ensure no one section of the network becomes too important and removes ReLu dead neurons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ell_2 regularization\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "loss_func = nn.BCELoss()\n",
    "loss = loss_func(torch.tensor([0.9]), torch.tensor([1.0]))\n",
    "l2_lambda = 0.001\n",
    "\n",
    "conv_layer = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=5)\n",
    "l2_penalty = l2_lambda * sum([(p**2).sum() for p in conv_layer.parameters()])\n",
    "loss_with_penalty = loss + l2_penalty\n",
    "\n",
    "linear_layer = nn.Linear(10, 16)\n",
    "l2_penalty = l2_lambda * sum([(p**2).sum() for p in linear_layer.parameters()])\n",
    "loss_with_penalty = loss + l2_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can use this [droput](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) remains the most important regularization method. \n",
    "\n",
    "During training, we will randomly drop neurons with a certain probability $p$ (commonly set to $p=0.5$). This forces the network to learn _redudant information in the training data_ increasing the model's robustness.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicitly this technique is performing ensemble learning as it learns different parts of feature-label relationship during each epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aside: There is a very out of place discussion on the difference between categorial and binary classification and the differ ways to implement in in PyTorch which we'll now code up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BCE (with Probs): 0.3711\n",
      "BCE (with Logits): 0.3711\n"
     ]
    }
   ],
   "source": [
    "# Binary Cross Entropy\n",
    "logits = torch.tensor([0.8])\n",
    "probs = torch.sigmoid(logits)\n",
    "target = torch.tensor([1.0])\n",
    "bce_loss_fn = nn.BCELoss()\n",
    "bce_logits_loss_fn = nn.BCEWithLogitsLoss()\n",
    "print(f\"BCE (with Probs): {bce_loss_fn(probs, target):.4f}\")\n",
    "print(f\"BCE (with Logits): {bce_logits_loss_fn(logits, target):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCE (with Probs): 0.5996\n",
      "CCE (with Logits): 0.5996\n"
     ]
    }
   ],
   "source": [
    "# Categorical Cross Entropy\n",
    "logits = torch.tensor([[1.5, 0.8, 2.1]])\n",
    "probs = torch.softmax(logits, dim=1)\n",
    "target = torch.tensor([2])\n",
    "cce_loss_fn = nn.NLLLoss()\n",
    "cce_logits_loss_fn = nn.CrossEntropyLoss()\n",
    "print(f\"CCE (with Probs): {cce_loss_fn(torch.log(probs), target):.4f}\")\n",
    "print(f\"CCE (with Logits): {cce_logits_loss_fn(logits, target):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
